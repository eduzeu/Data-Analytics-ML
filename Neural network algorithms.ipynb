{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=10, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(X, W1, W2):\n",
    "    # Input to the hidden layer\n",
    "    hidden_layer_input = np.dot(X, W1)\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "    # Input to the output layer\n",
    "    output_layer_input = np.dot(hidden_layer_output, W2)\n",
    "    output = output_layer_input\n",
    "\n",
    "    return hidden_layer_output, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_output_to_hidden(X, y, hidden_layer_output, output, W2, learning_rate=1e-4):\n",
    "    error = output - y\n",
    "    dW2 = np.dot(hidden_layer_output.T, error)\n",
    "    delta2 = np.dot(error, W2.T) * hidden_layer_output * (1 - hidden_layer_output)\n",
    "\n",
    "    # Update W2\n",
    "    W2 -= learning_rate * dW2\n",
    "\n",
    "    return delta2, W2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_hidden_to_input(X, delta2, W1, learning_rate=1e-4):\n",
    "    dW1 = np.dot(X.T, delta2)\n",
    "    W1 -= learning_rate * dW1\n",
    "\n",
    "    return W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Error: 19.477113757938216\n",
      "Epoch 100, Error: 9.04115464698605\n",
      "Epoch 200, Error: 6.07699467295619\n",
      "Epoch 300, Error: 4.673534705177801\n",
      "Epoch 400, Error: 3.9023445190769332\n",
      "Epoch 500, Error: 3.390672921262245\n",
      "Epoch 600, Error: 3.1266967008046285\n",
      "Epoch 700, Error: 2.97602550068025\n",
      "Epoch 800, Error: 2.751852053252308\n",
      "Epoch 900, Error: 3.205003423692017\n"
     ]
    }
   ],
   "source": [
    "def neural_network(X, y, n_hidden, learning_rate=1e-4, epochs=1000):\n",
    "    n_samples, n_features = X.shape\n",
    "    W1 = np.random.randn(n_features, n_hidden)\n",
    "    W2 = np.random.randn(n_hidden, 1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        hidden_layer_output, output = forward_propagation(X, W1, W2)\n",
    "\n",
    "        # Backward from Output to Hidden Layer\n",
    "        delta2, W2 = backward_output_to_hidden(X, y, hidden_layer_output, output, W2, learning_rate)\n",
    "\n",
    "        # Backward from Hidden to Input Layer\n",
    "        W1 = backward_hidden_to_input(X, delta2, W1, learning_rate)\n",
    "\n",
    "        # Calculating and printing error\n",
    "        error = np.mean(np.square(output - y)) /100000\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Error: {error}\")\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "# Training the neural network\n",
    "n_hidden = 4  # Number of neurons in the hidden layer\n",
    "W1, W2 = neural_network(X, y.reshape(-1, 1), n_hidden)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eduar\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.38288866e-01,  4.29304189e-01, -7.80874061e-01,  4.55950328e-01,\n",
       "       -5.11174858e-01, -7.83076657e-01,  1.79927016e-01,  4.14678906e-01,\n",
       "       -2.52950788e-02, -4.92336022e-01,  5.66645359e-01, -1.25922500e+00,\n",
       "        9.45371476e-02, -5.74716614e-01, -2.38185763e-01,  1.56765605e-01,\n",
       "        9.84654424e-03, -1.88861300e+00,  5.73259364e-01, -4.64679603e-01,\n",
       "        5.80732350e-01, -1.49650990e-01,  3.02418792e-01,  3.18222686e-01,\n",
       "        4.75611829e-01,  1.34925326e+00,  1.89527950e-01,  1.11934265e-01,\n",
       "       -1.61480928e-01,  1.10696479e+00,  1.23618255e+00,  1.85142249e-01,\n",
       "        2.27570545e-01,  4.09257720e-01,  2.91211094e-01,  5.82668258e-01,\n",
       "        5.20838854e-01,  2.14437722e-01,  4.99266994e-01,  2.48371981e-01,\n",
       "       -3.64843015e-01,  3.30378292e-02, -2.12654975e-01,  1.38208887e-01,\n",
       "        5.98555213e-01,  4.32337513e-01,  5.22892538e-01,  7.86549222e-01,\n",
       "       -3.71491245e-01,  2.39961436e-01,  5.66239588e-01,  3.02418792e-01,\n",
       "        1.62249540e-01, -1.82258258e+00, -1.22589881e+00, -6.81249651e-01,\n",
       "        5.41133497e-01,  1.46570694e-01, -1.28578924e-01,  6.39848773e-01,\n",
       "        4.84454077e-01,  3.02418792e-01,  3.25260846e-01, -8.02898230e-01,\n",
       "        1.28368499e+00, -5.11171202e-01,  2.17258939e+00, -1.38780982e+00,\n",
       "        9.71387524e-02,  4.99678499e-01,  4.54717067e-01, -2.72538623e-04,\n",
       "        3.31505130e-01,  2.98126880e-01,  3.66159530e-02,  3.02418792e-01,\n",
       "        5.90956573e-02,  2.54531510e-01,  1.38568368e-01,  4.53300431e-01,\n",
       "        1.27692456e-01,  3.46452530e-01,  3.39234460e-01,  3.23075841e-01,\n",
       "        3.02418792e-01,  7.71893882e-01,  1.33395385e-01, -9.11149075e-01,\n",
       "        1.03607886e+00,  5.36574417e-01, -3.63654904e-02,  2.22449135e-01,\n",
       "        3.02418792e-01,  3.50506010e-02, -2.56878823e-01, -4.53562581e-02,\n",
       "        1.83694619e-01,  6.46331497e-01, -3.43844924e-01, -3.04817872e-01])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Create the neural network model\n",
    "model = MLPRegressor(hidden_layer_sizes=(n_hidden,), max_iter=1000, learning_rate_init=1e-4, random_state=42)\n",
    "\n",
    " \n",
    "model.fit(X, y)\n",
    "\n",
    "\n",
    "predicted_y = model.predict(X)\n",
    "\n",
    "predicted_y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
